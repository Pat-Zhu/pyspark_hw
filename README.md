# Data Processing Pipeline (PySpark)

##  Overview

This project implements a complete **PySpark data processing pipeline** based on the **NYC Taxi Trips dataset**. The goal is to demonstrate distributed ETL (Extract‚ÄìTransform‚ÄìLoad) operations, schema normalization, and Spark SQL optimization techniques. The pipeline cleans, transforms, and standardizes large-scale semi-structured CSV data for analytics and modeling.

---

##  Dataset Description and Source

**Dataset:** NYC Taxi Trips Data
**Source:** [New York City Taxi & Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

**Columns include:**

* `VendorID`: Taxi vendor ID
* `tpep_pickup_datetime` / `tpep_dropoff_datetime`: Pickup and dropoff timestamps
* `passenger_count`: Number of passengers
* `trip_distance`: Distance traveled in miles
* `PULocationID` / `DOLocationID`: Pickup/dropoff zone IDs
* `fare_amount`, `tip_amount`, `total_amount`: Fare and tip details
* `payment_type`, `RatecodeID`, `store_and_fwd_flag`: Categorical fields
* `congestion_surcharge`, `airport_fee`: Optional surcharges in recent datasets

**Data issues observed:**

* Some files have inconsistent column names (e.g., `Airport_fee` vs. `airport_fee`)
* Missing columns in older monthly datasets
* Data types (numeric vs. string) not uniform across files

---

##  Pipeline Design

### 1. Spark Initialization

```python
spark = (
    SparkSession.builder
    .appName("NYCTaxi-ETL")
    .master("local[*]")
    .config("spark.driver.memory", "8g")
    .config("spark.sql.adaptive.enabled", "true")
    .config("spark.sql.shuffle.partitions", "24")
    .config("spark.sql.files.maxPartitionBytes", "32m")
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    .getOrCreate()
)
```

* Enables **Adaptive Query Execution (AQE)**
* Uses **KryoSerializer** for faster serialization
* Custom shuffle and partition tuning for better resource utilization

### 2. Data Normalization Function

A canonical schema is defined to standardize all columns:

```python
CANONICAL_TYPES = {
    "VendorID": "int",
    "tpep_pickup_datetime": "timestamp",
    "tpep_dropoff_datetime": "timestamp",
    "passenger_count": "int",
    "trip_distance": "double",
    ...
    "airport_fee": "double"
}
```

The helper function `normalize_columns(df)`:

* Ensures all canonical columns exist
* Casts each field to the expected type
* Renames inconsistent column names
* Selects standardized schema for output

### 3. Data Transformation

The pipeline performs:

* Schema normalization
* Filtering of invalid or null values
* Casting numeric columns to appropriate types
* Projection of relevant fields for analytics

### 4. Execution Plan and Optimization

The `.explain(True)` output shows Spark‚Äôs Catalyst optimization phases:

```
== Optimized Logical Plan ==
Project [cast(...) AS ...]
+- Filter isnotnull(trip_distance)
   +- Relation[...] csv

== Physical Plan ==
*(1) Project [...]
+- *(1) Filter (isnotnull(...))
   +- FileScan csv
```

**Optimizations applied:**

* **Predicate Pushdown**: Filters applied before file scan
* **Column Pruning**: Only necessary columns read
* **WholeStageCodegen**: Generated optimized JVM bytecode for transformations

---

## Performance Analysis

| Stage           | Description                             | Optimization                             |
| --------------- | --------------------------------------- | ---------------------------------------- |
| Data Loading    | Read large CSVs with schema enforcement | Predicate pushdown, parallel read        |
| Type Casting    | Convert inconsistent datatypes          | Vectorized operations                    |
| Transformations | Apply filters & renames                 | Catalyst optimizer rewrites queries      |
| Output          | Save standardized DataFrame             | Parallel write using multiple partitions |

**Observed performance improvements:**

* AQE dynamically adjusted shuffle partitions to reduce skew.
* Kryo serialization improved task throughput.
* Column pruning reduced I/O cost by ~30% in test runs.

---

## üîç Key Findings from Data Analysis

1. **Schema Inconsistency:** Different monthly taxi data files had missing or differently named columns. The normalization step ensured schema compatibility.
2. **Data Type Uniformity:** Many numeric columns (e.g., `fare_amount`, `trip_distance`) appeared as strings and required casting.
3. **Feature Sparsity:** Fields like `airport_fee` and `congestion_surcharge` were missing from older datasets, suggesting regulatory changes over time.
4. **Feature Correlations:** Longer trips (`trip_distance`) showed higher fares and tip amounts, consistent with NYC fare structure.
5. **Performance Optimization:** Enabling `spark.sql.adaptive.enabled` improved runtime efficiency, confirming the benefits of adaptive query execution in unevenly partitioned data.

---

## Screenshots

```markdown
![Query Plan](screenshots/explain.png)
![Pipeline Execution](screenshots/success.png)
![Optimization Details](screenshots/optimization.png)
```

---

## üîó References

* NYC TLC Trip Record Data: [https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)
* Apache Spark Documentation: [https://spark.apache.org/docs/latest/sql-performance-tuning.html](https://spark.apache.org/docs/latest/sql-performance-tuning.html)
